{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file creates a DataFrame containing stats and salaries for all NBA players in the specified years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NBA salary cap by year: https://basketball.realgm.com/nba/info/salary_cap\n",
    "- NBA player salary by year: http://www.espn.com/nba/salaries/_/year/2019/page/1/seasontype/1\n",
    "- NBA player stats: https://www.basketball-reference.com/leagues/NBA_2019_per_game.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata  # Used because basketball-reference has accented names, ESPN does not\n",
    "from nameparser import HumanName  # Used because some basketball-reference doesn't have suffixes\n",
    "import pickle  # Pickle dataframe to use in other project file\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # No limit on number of columns displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip accents from names\n",
    "def strip_accents(text):\n",
    "\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except NameError:  # unicode is a default on python 3 \n",
    "        pass\n",
    "\n",
    "    text = unicodedata.normalize('NFD', text)\\\n",
    "           .encode('ascii', 'ignore')\\\n",
    "           .decode(\"utf-8\")\n",
    "\n",
    "    return str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_list = list of category (pos, name, age, etc.)\n",
    "# category_str = name of category in string form\n",
    "# data_type = dtype of the category's stats (int, float, etc.)\n",
    "# iterator = i\n",
    "\n",
    "def get_single_data(category_list, category_str, data_type, iterator):\n",
    "    \n",
    "    if data_type == 'str':\n",
    "        if category_str == 'left':  # Names\n",
    "            category_list.append(strip_accents  # Need to strip accent characters from names\n",
    "                                 (full_table[iterator]\n",
    "                                  .find('td', {'class': 'left'})\n",
    "                                  .text\n",
    "                                  .upper()\n",
    "                                  .replace(',', '')\n",
    "                                  .replace('.', '')\n",
    "                                 )\n",
    "                                )\n",
    "        elif category_str == 'pos':  # Positions\n",
    "            category_list.append(full_table[iterator]\n",
    "                                 .find('td', {'data-stat': category_str})\n",
    "                                 .text\n",
    "                                 .upper()\n",
    "                                 .split('-')[0])  # Split any players with multiple positions, and only\n",
    "                                                  # keep their primary position (first string)\n",
    "                                                  # Ex: PF-SF becomes PF\n",
    "        else:\n",
    "            category_list.append(full_table[iterator].find('td', {'data-stat': category_str}).text.upper())\n",
    "            \n",
    "    elif data_type == 'int':\n",
    "        try:\n",
    "            category_list.append(int(full_table[iterator].find('td', {'data-stat': category_str}).text))\n",
    "        except:  # Fill any empty values with null\n",
    "            category_list.append(np.nan)\n",
    "\n",
    "    elif data_type == 'float':\n",
    "        try: \n",
    "            category_list.append(float(full_table[iterator].find('td', {'data-stat': category_str}).text))\n",
    "        except:  # Fill any empty values with null\n",
    "            category_list.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return dataframe with stats info for all players in the year passed\n",
    "def get_stats_data(year):\n",
    "    \n",
    "    # Initialize per_game stats lists\n",
    "    (name, pos, age, team_id, g, gs, mp_per_g, fg_per_g, fga_per_g, fg_pct, fg3_per_g, \n",
    "     fg3a_per_g, fg3_pct, fg2_per_g, fg2a_per_g, fg2_pct, efg_pct, ft_per_g, fta_per_g, \n",
    "     ft_pct, orb_per_g, drb_per_g, trb_per_g, ast_per_g,  stl_per_g, blk_per_g, tov_per_g, \n",
    "     pf_per_g, pts_per_g) = ([] for j in range(29))\n",
    "    \n",
    "    # Initialize per_minute stats lists\n",
    "    (fg_per_mp, fga_per_mp, fg3_per_mp, fg3a_per_mp, fg2_per_mp, fg2a_per_mp, ft_per_mp, fta_per_mp,\n",
    "     orb_per_mp, drb_per_mp, trb_per_mp, ast_per_mp, stl_per_mp, blk_per_mp, tov_per_mp, pf_per_mp,\n",
    "     pts_per_mp) = ([] for j in range(17))\n",
    "    \n",
    "    # Initialize per_poss stats lists\n",
    "    (fg_per_poss, fga_per_poss, fg3_per_poss, fg3a_per_poss, fg2_per_poss, fg2a_per_poss, ft_per_poss,\n",
    "     fta_per_poss, orb_per_poss, drb_per_poss, trb_per_poss, ast_per_poss, stl_per_poss, blk_per_poss,\n",
    "     tov_per_poss, pf_per_poss, pts_per_poss, off_rtg, def_rtg) = ([] for j in range(19))\n",
    "    \n",
    "    # Initialize totals stats lists\n",
    "    (fg, fga, fg3, fg3a, fg2, fg2a, ft, fta, orb, drb, trb, ast, stl, blk, tov, \n",
    "     pf, pts) = ([] for j in range(17))\n",
    "\n",
    "    # Initialize advanced stats lists\n",
    "    (per, ts_pct, fg3a_per_fga_pct, fta_per_fga_pct, orb_pct, drb_pct, trb_pct, ast_pct, \n",
    "     stl_pct, blk_pct, tov_pct, usg_pct, ows, dws, ws, ws_per_48, obpm, dbpm, bpm, \n",
    "     vorp) = ([] for j in range(20))\n",
    "    \n",
    "    stat_types = ['per_game', 'per_minute', 'per_poss', 'totals', 'advanced']\n",
    "    for stat_type in stat_types: \n",
    "        # Get HTML code from URL\n",
    "        url = 'https://www.basketball-reference.com/leagues/NBA_{}_{}.html'.format(str(year), stat_type)\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:  # 200 = success\n",
    "            print(response.status_code)\n",
    "            raise ValueError('Could not get response from URL!')\n",
    "            \n",
    "        # Contains all HTML code from URL\n",
    "        page = response.text\n",
    "        page_soup = soup(page, 'lxml')\n",
    "        \n",
    "        # Find all <tr tags that contain class 'full_table'\n",
    "        global full_table  # Global so that get_single_data() can access it\n",
    "        full_table = page_soup.findAll('tr', {'class': 'full_table'})\n",
    "        \n",
    "        for i in range(len(full_table)):  # len(full_table) = # of players in that season\n",
    "            if stat_type == 'per_game':\n",
    "                get_single_data(pos, 'pos', 'str', i)\n",
    "                get_single_data(name, 'left', 'str', i)\n",
    "                get_single_data(age, 'age', 'int', i)\n",
    "                get_single_data(team_id, 'team_id', 'str', i)\n",
    "                get_single_data(g, 'g', 'int', i)\n",
    "                get_single_data(gs, 'gs', 'int', i)\n",
    "                get_single_data(mp_per_g, 'mp_per_g', 'float', i)\n",
    "                get_single_data(fg_per_g, 'fg_per_g', 'float', i)\n",
    "                get_single_data(fga_per_g, 'fga_per_g', 'float', i)\n",
    "                get_single_data(fg_pct, 'fg_pct', 'float', i)\n",
    "                get_single_data(fg3_per_g, 'fg3_per_g', 'float', i)\n",
    "                get_single_data(fg3a_per_g, 'fg3a_per_g', 'float', i)\n",
    "                get_single_data(fg3_pct, 'fg3_pct', 'float', i)\n",
    "                get_single_data(fg2_per_g, 'fg2_per_g', 'float', i)\n",
    "                get_single_data(fg2a_per_g, 'fg2a_per_g', 'float', i)\n",
    "                get_single_data(fg2_pct, 'fg2_pct', 'float', i)\n",
    "                get_single_data(efg_pct, 'efg_pct', 'float', i)\n",
    "                get_single_data(ft_per_g, 'ft_per_g', 'float', i)\n",
    "                get_single_data(fta_per_g, 'fta_per_g', 'float', i)\n",
    "                get_single_data(ft_pct, 'ft_pct', 'float', i)\n",
    "                get_single_data(orb_per_g, 'orb_per_g', 'float', i)\n",
    "                get_single_data(drb_per_g, 'drb_per_g', 'float', i)\n",
    "                get_single_data(trb_per_g, 'trb_per_g', 'float', i)\n",
    "                get_single_data(ast_per_g, 'ast_per_g', 'float', i)\n",
    "                get_single_data(stl_per_g, 'stl_per_g', 'float', i)\n",
    "                get_single_data(blk_per_g, 'blk_per_g', 'float', i)\n",
    "                get_single_data(tov_per_g, 'tov_per_g', 'float', i)\n",
    "                get_single_data(pf_per_g, 'pf_per_g', 'float', i)\n",
    "                get_single_data(pts_per_g, 'pts_per_g', 'float', i)\n",
    "            elif stat_type == 'per_minute':\n",
    "                get_single_data(fg_per_mp, 'fg_per_mp', 'float', i)\n",
    "                get_single_data(fga_per_mp, 'fga_per_mp', 'float', i)\n",
    "                get_single_data(fg3_per_mp, 'fg3_per_mp', 'float', i)\n",
    "                get_single_data(fg3a_per_mp, 'fg3a_per_mp', 'float', i)\n",
    "                get_single_data(fg2_per_mp, 'fg2_per_mp', 'float', i)\n",
    "                get_single_data(fg2a_per_mp, 'fg2a_per_mp', 'float', i)\n",
    "                get_single_data(ft_per_mp, 'ft_per_mp', 'float', i)\n",
    "                get_single_data(fta_per_mp, 'fta_per_mp', 'float', i)\n",
    "                get_single_data(orb_per_mp, 'orb_per_mp', 'float', i)\n",
    "                get_single_data(drb_per_mp, 'drb_per_mp', 'float', i)\n",
    "                get_single_data(trb_per_mp, 'trb_per_mp', 'float', i)\n",
    "                get_single_data(ast_per_mp, 'ast_per_mp', 'float', i)\n",
    "                get_single_data(stl_per_mp, 'stl_per_mp', 'float', i)\n",
    "                get_single_data(blk_per_mp, 'blk_per_mp', 'float', i)\n",
    "                get_single_data(tov_per_mp, 'tov_per_mp', 'float', i)\n",
    "                get_single_data(pf_per_mp, 'pf_per_mp', 'float', i)\n",
    "                get_single_data(pts_per_mp, 'pts_per_mp', 'float', i)\n",
    "            elif stat_type == 'per_poss':\n",
    "                get_single_data(fg_per_poss, 'fg_per_poss', 'float', i)\n",
    "                get_single_data(fga_per_poss, 'fga_per_poss', 'float', i)\n",
    "                get_single_data(fg3_per_poss, 'fg3_per_poss', 'float', i)\n",
    "                get_single_data(fg3a_per_poss, 'fg3a_per_poss', 'float', i)\n",
    "                get_single_data(fg2_per_poss, 'fg2_per_poss', 'float', i)\n",
    "                get_single_data(fg2a_per_poss, 'fg2a_per_poss', 'float', i)\n",
    "                get_single_data(ft_per_poss, 'ft_per_poss', 'float', i)\n",
    "                get_single_data(fta_per_poss, 'fta_per_poss', 'float', i)\n",
    "                get_single_data(orb_per_poss, 'orb_per_poss', 'float', i)\n",
    "                get_single_data(drb_per_poss, 'drb_per_poss', 'float', i)\n",
    "                get_single_data(trb_per_poss, 'trb_per_poss', 'float', i)\n",
    "                get_single_data(ast_per_poss, 'ast_per_poss', 'float', i)\n",
    "                get_single_data(stl_per_poss, 'stl_per_poss', 'float', i)\n",
    "                get_single_data(blk_per_poss, 'blk_per_poss', 'float', i)\n",
    "                get_single_data(tov_per_poss, 'tov_per_poss', 'float', i)\n",
    "                get_single_data(pf_per_poss, 'pf_per_poss', 'float', i)\n",
    "                get_single_data(pts_per_poss, 'pts_per_poss', 'float', i)\n",
    "                get_single_data(off_rtg, 'off_rtg', 'int', i)\n",
    "                get_single_data(def_rtg, 'def_rtg', 'int', i)                \n",
    "            elif stat_type == 'totals':\n",
    "                get_single_data(fg, 'fg', 'int', i)\n",
    "                get_single_data(fga, 'fga', 'int', i)\n",
    "                get_single_data(fg3, 'fg3', 'int', i)\n",
    "                get_single_data(fg3a, 'fg3a', 'int', i)\n",
    "                get_single_data(fg2, 'fg2', 'int', i)\n",
    "                get_single_data(fg2a, 'fg2a', 'int', i)\n",
    "                get_single_data(ft, 'ft', 'int', i)\n",
    "                get_single_data(fta, 'fta', 'int', i)\n",
    "                get_single_data(orb, 'orb', 'int', i)\n",
    "                get_single_data(drb, 'drb', 'int', i)\n",
    "                get_single_data(trb, 'trb', 'int', i)\n",
    "                get_single_data(ast, 'ast', 'int', i)\n",
    "                get_single_data(stl, 'stl', 'int', i)\n",
    "                get_single_data(blk, 'blk', 'int', i)\n",
    "                get_single_data(tov, 'tov', 'int', i)\n",
    "                get_single_data(pf, 'pf', 'int', i)\n",
    "                get_single_data(pts, 'pts', 'int', i)\n",
    "            elif stat_type == 'advanced':\n",
    "                get_single_data(per, 'per', 'float', i)\n",
    "                get_single_data(ts_pct, 'ts_pct', 'float', i)\n",
    "                get_single_data(fg3a_per_fga_pct, 'fg3a_per_fga_pct', 'float', i)\n",
    "                get_single_data(fta_per_fga_pct, 'fta_per_fga_pct', 'float', i)\n",
    "                get_single_data(orb_pct, 'orb_pct', 'float', i)\n",
    "                get_single_data(drb_pct, 'drb_pct', 'float', i)\n",
    "                get_single_data(trb_pct, 'trb_pct', 'float', i)\n",
    "                get_single_data(ast_pct, 'ast_pct', 'float', i)\n",
    "                get_single_data(stl_pct, 'stl_pct', 'float', i)\n",
    "                get_single_data(blk_pct, 'blk_pct', 'float', i)\n",
    "                get_single_data(tov_pct, 'tov_pct', 'float', i)\n",
    "                get_single_data(usg_pct, 'usg_pct', 'float', i)\n",
    "                get_single_data(ows, 'ows', 'float', i)\n",
    "                get_single_data(dws, 'dws', 'float', i)\n",
    "                get_single_data(ws, 'ws', 'float', i)\n",
    "                get_single_data(ws_per_48, 'ws_per_48', 'float', i)\n",
    "                get_single_data(obpm, 'obpm', 'float', i)\n",
    "                get_single_data(dbpm, 'dbpm', 'float', i)\n",
    "                get_single_data(bpm, 'bpm', 'float', i)\n",
    "                get_single_data(vorp, 'vorp', 'float', i)\n",
    "    \n",
    "    # Make dict to contain all stats data\n",
    "    stats_dict = (\n",
    "                    {\n",
    "                        # Per game stats \n",
    "                        'name': name, 'pos': pos, 'age': age, 'team_id': team_id, 'g': g, 'gs': gs, \n",
    "                        'mp_per_g': mp_per_g, 'fg_per_g': fg_per_g, 'fga_per_g': fga_per_g, \n",
    "                        'fg_pct': fg_pct, 'fg3_per_g': fg3_per_g, 'fg3a_per_g': fg3a_per_g,\n",
    "                        'fg3_pct': fg3_pct, 'fg2_per_g': fg2_per_g, 'fg2a_per_g': fg2a_per_g, \n",
    "                        'fg2_pct': fg2_pct, 'efg_pct': efg_pct,'ft_per_g': ft_per_g, \n",
    "                        'fta_per_g': fta_per_g, 'ft_pct': ft_pct, 'orb_per_g': orb_per_g, \n",
    "                        'drb_per_g': drb_per_g,'trb_per_g': trb_per_g, 'ast_per_g': ast_per_g, \n",
    "                        'stl_per_g': stl_per_g, 'blk_per_g': blk_per_g, 'tov_per_g': tov_per_g,\n",
    "                        'pf_per_g': pf_per_g, 'pts_per_g': pts_per_g, \n",
    "                        # Per 36 minutes stats\n",
    "                        'fg_per_mp': fg_per_mp, 'fga_per_mp': fga_per_mp, 'fg3_per_mp': fg3_per_mp, \n",
    "                        'fg3a_per_mp': fg3a_per_mp, 'fg2_per_mp': fg2_per_mp, \n",
    "                        'fg2a_per_mp': fg2a_per_mp, 'ft_per_mp': ft_per_mp, 'fta_per_mp': fta_per_mp, \n",
    "                        'orb_per_mp': orb_per_mp, 'drb_per_mp': drb_per_mp, 'trb_per_mp': trb_per_mp, \n",
    "                        'ast_per_mp': ast_per_mp, 'stl_per_mp': stl_per_mp, 'blk_per_mp': blk_per_mp, \n",
    "                        'tov_per_mp': tov_per_mp, 'pf_per_mp': pf_per_mp, 'pts_per_mp': pts_per_mp,\n",
    "                        # Per 100 possession stats\n",
    "                        'fg_per_poss': fg_per_poss, 'fga_per_poss': fga_per_poss, \n",
    "                        'fg3_per_poss': fg3_per_poss, 'fg3a_per_poss': fg3a_per_poss, \n",
    "                        'fg2_per_poss': fg2_per_poss, 'fg2a_per_poss': fg2a_per_poss, \n",
    "                        'ft_per_poss': ft_per_poss, 'fta_per_poss': fta_per_poss, \n",
    "                        'orb_per_poss': orb_per_poss, 'drb_per_poss': drb_per_poss, \n",
    "                        'trb_per_poss': trb_per_poss, 'ast_per_poss': ast_per_poss, \n",
    "                        'stl_per_poss': stl_per_poss, 'blk_per_poss': blk_per_poss, \n",
    "                        'tov_per_poss': tov_per_poss, 'pf_per_poss': pf_per_poss, \n",
    "                        'pts_per_poss': pts_per_poss, 'off_rtg': off_rtg, 'def_rtg': def_rtg,\n",
    "                        # Stat totals\n",
    "                        'fg': fg, 'fga': fga, 'fg3': fg3, 'fg3a': fg3a, 'fg2': fg2, 'fg2a': fg2a, \n",
    "                        'ft': ft, 'fta': fta, 'orb': orb, 'drb': drb, 'trb': trb, 'ast': ast, \n",
    "                        'stl': stl, 'blk': blk, 'tov': tov, 'pf': pf, 'pts': pts,\n",
    "                        # Advanced stats\n",
    "                        'per': per, 'ts_pct': ts_pct, 'fg3a_per_fga_pct': fg3a_per_fga_pct,\n",
    "                        'fta_per_fga_pct': fta_per_fga_pct, 'orb_pct': orb_pct, 'drb_pct': drb_pct, \n",
    "                        'trb_pct': trb_pct, 'ast_pct': ast_pct, 'stl_pct': stl_pct, 'blk_pct': blk_pct, \n",
    "                        'tov_pct': tov_pct, 'usg_pct': usg_pct, 'ows': ows, 'dws': dws, 'ws': ws,\n",
    "                        'ws_per_48': ws_per_48, 'obpm': obpm, 'dbpm': dbpm, 'bpm': bpm, 'vorp': vorp\n",
    "                    }\n",
    "                )\n",
    "    stats_df = pd.DataFrame(stats_dict)\n",
    "    stats_df.insert(0, 'year', year)      # Insert a column with the year\n",
    "    \n",
    "    return stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return dict in {'player_name': salary} format\n",
    "def get_salary_data(year):\n",
    "    \n",
    "    # Dictionary to be filled with player names and salaries\n",
    "    salary_info = {}\n",
    "    \n",
    "    # Get HTML code from URL. By default, start from page 1\n",
    "    url = 'http://www.espn.com/nba/salaries/_/year/{}/page/1/seasontype/1'.format(str(year))\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:  # 200 = success\n",
    "        print(response.status_code)\n",
    "        raise ValueError('Could not get response from URL!')\n",
    "     \n",
    "    # Contains all HTML code from URL\n",
    "    page = response.text\n",
    "    page_soup = soup(page, 'lxml')  # Make into soup object\n",
    "    \n",
    "    # Find number of pages\n",
    "    num_pages = int(page_soup.\n",
    "                find('div', {'class': 'page-numbers'})  # Find string where '1 of last_page' is\n",
    "                .text                                   # Convert to text\n",
    "                .split()                                # Split string at spaces, and get last_page\n",
    "                [-1])\n",
    "    \n",
    "    for page_num in range(num_pages):\n",
    "        page_num += 1  # Index starts at 1\n",
    "        # Get HTML code from URL. By default, start from page 1\n",
    "        url = ('http://www.espn.com/nba/salaries/_/year/{}/page/{}/seasontype/1'\n",
    "               .format(str(year), str(page_num))\n",
    "              )\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:  # 200 = success\n",
    "            print(response.status_code)\n",
    "            raise ValueError('Could not get response from URL!')\n",
    "     \n",
    "        # Contains all HTML code from URL\n",
    "        page = response.text\n",
    "        page_soup = soup(page, 'lxml')  # Make into soup object\n",
    "        \n",
    "        # Find all <tr tags that contain class 'oddrow' or 'evenrow'\n",
    "        table = page_soup.findAll('tr', {'class': ['oddrow', 'evenrow']})\n",
    "\n",
    "        # Iterate thru every row in the salary info table\n",
    "        for i in range(len(table)):\n",
    "            # key is name\n",
    "            # value is salary\n",
    "            name = table[i].find('a').text.strip().upper().replace(',', '').replace('.', '')\n",
    "            \n",
    "            if (HumanName(name).first + ' ' + HumanName(name).last) != name:\n",
    "                name = HumanName(name).first + ' ' + HumanName(name).last\n",
    "            \n",
    "            value = int(table[i].findAll('td')[-1]  # The last 'td' tag in the table has the salary\n",
    "                        .text                       # Get string from HTML\n",
    "                        .replace('$', '')           # Format so data can become an int\n",
    "                        .replace(',', ''))\n",
    "            salary_info[name] = value\n",
    "    \n",
    "    return salary_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get salary cap for specified year\n",
    "def get_salary_cap(year):\n",
    "    \n",
    "    url = 'https://basketball.realgm.com/nba/info/salary_cap'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:  # 200 = success\n",
    "        print(response.status_code)\n",
    "        raise ValueError('Could not get response from URL!')\n",
    "\n",
    "    # Contains all HTML code from URL\n",
    "    page = response.text\n",
    "    page_soup = soup(page, 'lxml')\n",
    "\n",
    "    # Find <tbody tag, inside that find all items with <tr tag\n",
    "    basketball_compact = page_soup.find('tbody').findAll('tr')\n",
    "    \n",
    "    # Create dict where we can fill dict[season] = salary_cap\n",
    "    salary_cap_dict = {}\n",
    "    \n",
    "    for i in range(len(basketball_compact)):\n",
    "        # Season is the year\n",
    "        season = int(basketball_compact[i]\n",
    "                     .findAll('td')[2]  # The third <td tag has the season year\n",
    "                     .text\n",
    "                     .split('-')[-1])\n",
    "        salary_cap = int(basketball_compact[i]\n",
    "                         .findAll('td')[3]  # The fourth <td tag has the season salary cap\n",
    "                         .text\n",
    "                         .replace('$', '')\n",
    "                         .replace(',', ''))\n",
    "        # Store salary cap to the corresponding season\n",
    "        salary_cap_dict[season] = salary_cap\n",
    "        \n",
    "    return salary_cap_dict[year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill passed dataframe with salaries from passed dictionary\n",
    "def combine_data(year, stats_df, salaries_dict, salary_cap):\n",
    "    \n",
    "    stats_df.insert(1, 'salary', np.nan)          # Fill 'salary' column with NaN until data is added\n",
    "    stats_df.insert(2, 'salary cap', salary_cap)  # Fill with year's salary_cap\n",
    "    stats_df.insert(len(stats_df.columns), 'salary ratio', np.nan)    # Fill 'salary ratio' column with NaN until data is added\n",
    "    \n",
    "    index = 0  # Keep index of dataframe, so it can be used for changing values\n",
    "    \n",
    "    for name in stats_df['name']:\n",
    "        if name in salaries_dict:\n",
    "            # Fill salary for each player\n",
    "            stats_df.at[index, 'salary'] = salaries_dict[name]\n",
    "            \n",
    "            # Salary ratio = salary / salary_cap\n",
    "            # Salary ratio is a means to normalize player salaries across different seasons\n",
    "            # It is required because thes salary cap increases every year\n",
    "            stats_df.at[index, 'salary ratio'] = salaries_dict[name] / salary_cap\n",
    "        index += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all required data for a list of specified years. Return a df with all data\n",
    "def get_data(years_list):\n",
    "    \n",
    "    df = pd.DataFrame()  # Create empty df that will be appended with ever year's data\n",
    "    \n",
    "    for year in years_list:\n",
    "        stats_df = get_stats_data(year)\n",
    "        salaries_dict = get_salary_data(year)\n",
    "        salary_cap = get_salary_cap(year)\n",
    "        combine_data(year, stats_df, salaries_dict, salary_cap)\n",
    "        df = df.append(stats_df)\n",
    "     \n",
    "    # Drop any rows that have NaN values\n",
    "    # These rows have either missing salary data from ESPN or the\n",
    "    # player didn't have enough stats to calculate advance stats             \n",
    "    return df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTER WHICH YEARS YOU WANT HERE. ONLY PART OF THIS NOTEBOOK THAT NEEDS TO BE TOUCHED!!!\n",
    "# Earliest year is 2000 (1999-2000 season)\n",
    "# Create df with data from the specified years\n",
    "df = get_data([2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, \n",
    "               2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pickle dataframe to use in other project file\n",
    "with open('stats_df.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(df, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
